{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ğŸ”¬ SciCO - The Zotero Library RAG System\n",
    "\n",
    "Welcome to **SciCO** (Scientific Co-worker)! This interactive tutorial will guide you through:\n",
    "\n",
    "1. **Configuration** - Setting up your environment\n",
    "2. **Zotero Integration** - Connecting to your library and retrieving metadata\n",
    "3. **PDF Processing** - Converting PDFs to searchable markdown\n",
    "4. **Text Chunking** - Breaking documents into semantic pieces\n",
    "5. **Vector Storage** - Creating embeddings for semantic search\n",
    "6. **Retrieval** - Querying your knowledge base\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** combines:\n",
    "- **Vector databases** for semantic search\n",
    "- **Your documents** as the knowledge source\n",
    "- **LLMs** for intelligent question answering\n",
    "\n",
    "This allows you to ask questions about your scientific papers and get answers grounded in your actual sources!"
   ],
   "id": "f316f524255b5f4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## ğŸ“‹ Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "\n",
    "âœ… **Ollama** installed and running ([https://ollama.ai](https://ollama.ai))  \n",
    "âœ… **Zotero** with a populated library  \n",
    "âœ… A `.env` file with required variables (see below)  \n",
    "âœ… Python packages installed (`pip install -r requirements.txt`)\n"
   ],
   "id": "prerequisite_section"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 1ï¸âƒ£ Configuration\n",
    "\n",
    "The project requires a `.env` file in your project root with these variables:"
   ],
   "id": "1c4ed18c0ba44d58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ğŸ“ Required Environment Variables"
   ],
   "id": "b7a41fcea696661e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T14:40:42.901822Z",
     "start_time": "2025-10-22T14:40:42.898941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example .env file structure:\n",
    "# Copy this to your .env file and fill in the paths\n",
    "\n",
    "example_env = \"\"\"\n",
    "# Name of the collection in your Zotero library\n",
    "COLLECTION_NAME='Your Collection Name'\n",
    "\n",
    "# Path where markdown files will be saved\n",
    "MARKDOWN_FOLDER_PATH='/path/to/markdown/output'\n",
    "\n",
    "# Path to your Zotero data folder (contains zotero.sqlite)\n",
    "ZOTERO_LIBRARY_PATH='/path/to/Zotero/data'\n",
    "\n",
    "# Path to the ChromaDB index file (should end in .db)\n",
    "INDEX_PATH='/path/to/index/chroma.db'\n",
    "\n",
    "# (Optional) For testing\n",
    "TEST_PDF_PATH='/path/to/test/paper.pdf'\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸ“„ Example .env configuration:\")\n",
    "print(example_env)"
   ],
   "id": "e05060f57367368e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Example .env configuration:\n",
      "\n",
      "# Name of the collection in your Zotero library\n",
      "COLLECTION_NAME='Your Collection Name'\n",
      "\n",
      "# Path where markdown files will be saved\n",
      "MARKDOWN_FOLDER_PATH='/path/to/markdown/output'\n",
      "\n",
      "# Path to your Zotero data folder (contains zotero.sqlite)\n",
      "ZOTERO_LIBRARY_PATH='/path/to/Zotero/data'\n",
      "\n",
      "# Path to the ChromaDB index file (should end in .db)\n",
      "INDEX_PATH='/path/to/index/chroma.db'\n",
      "\n",
      "# (Optional) For testing\n",
      "TEST_PDF_PATH='/path/to/test/paper.pdf'\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ğŸ”§ Import Dependencies and Setup"
   ],
   "id": "import_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T14:40:42.963071Z",
     "start_time": "2025-10-22T14:40:42.929672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project source to path\n",
    "project_src = Path.cwd()\n",
    "if str(project_src) not in sys.path:\n",
    "    sys.path.insert(0, str(project_src))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Dependencies imported successfully!\")"
   ],
   "id": "bb457cc55e456102",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dependencies imported successfully!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ğŸ¥ Health Check: Verify Ollama is Running"
   ],
   "id": "health_check_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T14:40:42.990056Z",
     "start_time": "2025-10-22T14:40:42.981176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ensure_ollama_running(host: str = \"127.0.0.1\", port: int = 11434, timeout: float = 2.0) -> dict:\n",
    "    \"\"\"Check if Ollama is running and return status info.\"\"\"\n",
    "    base_url = f\"http://{host}:{port}\"\n",
    "    try:\n",
    "        resp = requests.get(f\"{base_url}/api/version\", timeout=timeout)\n",
    "        if resp.status_code == 200:\n",
    "            version_info = resp.json()\n",
    "            return {\n",
    "                'status': 'running',\n",
    "                'url': base_url,\n",
    "                'version': version_info.get('version', 'unknown')\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'message': f\"Ollama responded with status {resp.status_code}\"\n",
    "            }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\n",
    "            'status': 'not_running',\n",
    "            'message': f\"Cannot reach Ollama at {base_url}\",\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "# Run health check\n",
    "ollama_status = ensure_ollama_running()\n",
    "\n",
    "if ollama_status['status'] == 'running':\n",
    "    display(HTML(\n",
    "        f\"<div style='padding:10px; background-color:#1a4d2e; border-left:4px solid #28a745; border-radius:4px; color:#ffffff;'>\"\n",
    "        f\"<strong>âœ… Ollama is running!</strong><br>\"\n",
    "        f\"ğŸ”— URL: {ollama_status['url']}<br>\"\n",
    "        f\"ğŸ“¦ Version: {ollama_status['version']}\"\n",
    "        f\"</div>\"))\n",
    "else:\n",
    "    display(HTML(\n",
    "        f\"<div style='padding:10px; background-color:#5c1a1a; border-left:4px solid #dc3545; border-radius:4px; color:#ffffff;'>\"\n",
    "        f\"<strong>âŒ Ollama is NOT running!</strong><br>\"\n",
    "        f\"âš ï¸ {ollama_status.get('message', 'Unknown error')}<br>\"\n",
    "        f\"<em>Please start Ollama before continuing.</em>\"\n",
    "        f\"</div>\"))\n",
    "    raise RuntimeError(\"Ollama must be running to use this notebook.\")\n"
   ],
   "id": "1b5da5ba20bbbb07",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='padding:10px; background-color:#1a4d2e; border-left:4px solid #28a745; border-radius:4px; color:#ffffff;'><strong>âœ… Ollama is running!</strong><br>ğŸ”— URL: http://127.0.0.1:11434<br>ğŸ“¦ Version: 0.12.5</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ğŸ“Š Verify Configuration"
   ],
   "id": "verify_config_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T14:40:43.038102Z",
     "start_time": "2025-10-22T14:40:43.035168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if all required environment variables are set\n",
    "required_vars = ['COLLECTION_NAME', 'MARKDOWN_FOLDER_PATH', 'ZOTERO_LIBRARY_PATH', 'INDEX_PATH']\n",
    "config_status = {}\n",
    "\n",
    "print(\"ğŸ” Configuration Status:\\n\")\n",
    "for var in required_vars:\n",
    "    value = os.getenv(var)\n",
    "    config_status[var] = value\n",
    "    status = \"âœ…\" if value else \"âŒ\"\n",
    "    print(f\"{status} {var}: {value if value else 'NOT SET'}\")\n",
    "\n",
    "all_set = all(config_status.values())\n",
    "if all_set:\n",
    "    print(\"\\nâœ… All required variables are configured!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Some variables are missing. Please update your .env file.\")"
   ],
   "id": "verify_config_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Configuration Status:\n",
      "\n",
      "âœ… COLLECTION_NAME: scico-test\n",
      "âœ… MARKDOWN_FOLDER_PATH: example/markdown-library\n",
      "âœ… ZOTERO_LIBRARY_PATH: /home/soenke/Zotero\n",
      "âœ… INDEX_PATH: example/zotero-vector-storage.db\n",
      "\n",
      "âœ… All required variables are configured!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 2ï¸âƒ£ Zotero Integration\n",
    "\n",
    "Let's connect to your Zotero database and explore what's inside!"
   ],
   "id": "7cdf4e67339e51a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T14:40:43.330136Z",
     "start_time": "2025-10-22T14:40:43.084632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from legacy.ZoteroIntegration import ZoteroMetadataRetriever\n",
    "\n",
    "# Initialize the Zotero connection\n",
    "zotero_path = Path(os.getenv('ZOTERO_LIBRARY_PATH'))\n",
    "retriever = ZoteroMetadataRetriever(zotero_path)\n",
    "\n",
    "print(\"ğŸ”Œ Connecting to Zotero database...\")\n",
    "retriever.initialize()\n",
    "print(\"âœ… Connected successfully!\")\n",
    "print(f\"ğŸ“‚ Database path: {retriever.config.sqlite_path}\")"
   ],
   "id": "zotero_connect_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”Œ Connecting to Zotero database...\n",
      "âœ… Connected successfully!\n",
      "ğŸ“‚ Database path: /home/soenke/Zotero/zotero.sqlite\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ğŸ“š Explore Collections and PDFs"
   ],
   "id": "explore_collections_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T14:40:43.414710Z",
     "start_time": "2025-10-22T14:40:43.336225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get PDFs from the configured collection\n",
    "collection_name = os.getenv('COLLECTION_NAME')\n",
    "print(f\"ğŸ“– Retrieving PDFs from collection: '{collection_name}'\\n\")\n",
    "\n",
    "pdfs = retriever.get_pdfs_in_collection(collection_name)\n",
    "\n",
    "if pdfs:\n",
    "    print(f\"âœ… Found {len(pdfs)} PDF(s) in this collection\\n\")\n",
    "    print(\"ğŸ“„ First 3 PDFs:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, pdf in enumerate(pdfs[:3], 1):\n",
    "        print(f\"\\n{i}. {pdf['pdf_name']}\")\n",
    "        print(f\"   Citation Key: {pdf['citationkey'] or 'None'}\")\n",
    "        print(f\"   Item ID: {pdf['itemID']}\")\n",
    "        print(f\"   Path: {pdf['pdf_path'] or 'Not found in storage'}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ No PDFs found in collection '{collection_name}'\")\n",
    "    print(\"Tip: Make sure your collection name matches exactly (case-sensitive)\")"
   ],
   "id": "get_pdfs_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– Retrieving PDFs from collection: 'scico-test'\n",
      "\n",
      "âœ… Found 1 PDF(s) in this collection\n",
      "\n",
      "ğŸ“„ First 3 PDFs:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Wegner et al. - 2023 - Complexity measures for EEG microstate sequences - concepts and algorithms.pdf\n",
      "   Citation Key: wegnerComplexityMeasuresEEG2023\n",
      "   Item ID: 289\n",
      "   Path: /home/soenke/Zotero/storage/9N8E7TQU/Wegner et al. - 2023 - Complexity measures for EEG microstate sequences - concepts and algorithms.pdf\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ğŸ” Deep Dive: Get Full Metadata for a PDF"
   ],
   "id": "metadata_deep_dive_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T14:40:43.440986Z",
     "start_time": "2025-10-22T14:40:43.421323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's examine the full metadata for the first PDF (if available)\n",
    "def metadata_parser_for_chunks(metadata):\n",
    "    return {\n",
    "        'title': metadata.get('title', 'N/A'),\n",
    "        'authors': metadata.get('authors', 'N/A'),\n",
    "        'year': metadata.get('year', 'N/A'),\n",
    "        'doi': metadata.get('doi', 'N/A'),\n",
    "        'url': metadata.get('url', 'N/A'),\n",
    "        'citation_key': metadata.get('citation_key', 'N/A'),\n",
    "        'collections' : '; '.join([col['name'] for col in metadata.get('collections', [])] if metadata.get('collections') else 'N/A'),\n",
    "        'content_type': 'pdf-zotero',\n",
    "    }\n",
    "\n",
    "if pdfs and pdfs[0]['pdf_path']:\n",
    "    sample_pdf_path = Path(pdfs[0]['pdf_path'])\n",
    "    print(f\"ğŸ”¬ Analyzing: {sample_pdf_path.name}\\n\")\n",
    "    \n",
    "    metadata = retriever.get_metadata_for_pdf(sample_pdf_path)\n",
    "    \n",
    "    if metadata:\n",
    "        zotero_metadata = metadata_parser_for_chunks(metadata)\n",
    "        print(\"ğŸ“Š Full Metadata:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nğŸ“– Title: {metadata.get('title', 'N/A')}\")\n",
    "        print(f\"\\nâœï¸ Authors: {metadata.get('authors', 'N/A')}\")\n",
    "        print(f\"\\nğŸ“… Year: {metadata.get('year', 'N/A')}\")\n",
    "        print(f\"\\nğŸ”— DOI: {metadata.get('doi', 'N/A')}\")\n",
    "        print(f\"\\nğŸŒ URL: {metadata.get('url', 'N/A')}\")\n",
    "        print(f\"\\nğŸ“° Publication: {metadata.get('publication_title', 'N/A')}\")\n",
    "        \n",
    "        if metadata.get('abstract'):\n",
    "            abstract = metadata['abstract']\n",
    "            print(f\"\\nğŸ“ Abstract: {abstract[:200]}...\" if len(abstract) > 200 else f\"\\nğŸ“ Abstract: {abstract}\")\n",
    "        \n",
    "        if metadata.get('tags'):\n",
    "            print(f\"\\nğŸ·ï¸ Tags: {', '.join(metadata['tags'])}\")\n",
    "        \n",
    "        if metadata.get('collections'):\n",
    "            print(f\"\\nğŸ“ Collections:\")\n",
    "            for coll in metadata['collections']:\n",
    "                print(f\"   - {coll['name']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "    else:\n",
    "        print(\"âš ï¸ Could not retrieve metadata\")\n",
    "else:\n",
    "    print(\"âš ï¸ No valid PDF path found for analysis\")\n",
    "    print(\"Tip: Make sure PDFs exist in your Zotero storage folder\")"
   ],
   "id": "metadata_details_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ Analyzing: Wegner et al. - 2023 - Complexity measures for EEG microstate sequences - concepts and algorithms.pdf\n",
      "\n",
      "ğŸ“Š Full Metadata:\n",
      "================================================================================\n",
      "\n",
      "ğŸ“– Title: Complexity measures for EEG microstate sequences - concepts and algorithms\n",
      "\n",
      "âœï¸ Authors: Wegner, Frederic von; Wiemers, Milena; Hermann, Gesine; TÃ¶dt, Inken; Tagliazucchi, Enzo; Laufs, Helmut\n",
      "\n",
      "ğŸ“… Year: 2023\n",
      "\n",
      "ğŸ”— DOI: 10.21203/rs.3.rs-2878411/v1\n",
      "\n",
      "ğŸŒ URL: https://www.researchsquare.com/article/rs-2878411/v1\n",
      "\n",
      "ğŸ“° Publication: None\n",
      "\n",
      "ğŸ“ Abstract: EEG microstate sequence analysis quantifies properties of ongoing brain electrical activity which is known to exhibit complex dynamics across many time scales. In this report we review recent developm...\n",
      "\n",
      "ğŸ“ Collections:\n",
      "   - PCI-From-Resting-State-Reconstruction\n",
      "   - scico-test\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 3ï¸âƒ£ PDF to Markdown Conversion\n",
    "\n",
    "Now let's convert a PDF to structured Markdown using the `marker` library with Ollama."
   ],
   "id": "pdf_conversion_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T14:40:49.273859Z",
     "start_time": "2025-10-22T14:40:43.478180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.Tools.PdfToMarkdown import convert_pdf_to_markdown\n",
    "\n",
    "# We'll use the first PDF from our collection (if available)\n",
    "if pdfs and pdfs[0]['pdf_path']:\n",
    "    pdf_path = pdfs[0]['pdf_path']\n",
    "    output_folder = os.getenv('MARKDOWN_FOLDER_PATH')\n",
    "    \n",
    "    print(f\"ğŸ“„ Converting PDF: {Path(pdf_path).name}\")\n",
    "    print(f\"ğŸ“‚ Output folder: {output_folder}\")\n",
    "    print(\"\\nâ³ This may take a few minutes depending on PDF size...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        # Find the generated markdown file\n",
    "        pdf_name = Path(pdf_path).stem\n",
    "        markdown_path = Path(output_folder) / pdf_name / f\"{pdf_name}.md\"\n",
    "\n",
    "        # Convert PDF to markdown\n",
    "        try:\n",
    "            convert_pdf_to_markdown(pdf_path=pdf_path, output_path=output_folder)\n",
    "        except FileExistsError as e:\n",
    "            print('File is already processed')\n",
    "\n",
    "        if markdown_path.exists():\n",
    "            print(f\"\\nâœ… Conversion successful!\")\n",
    "            print(f\"ğŸ“ Markdown file: {markdown_path}\")\n",
    "            \n",
    "            # Show a preview\n",
    "            with open(markdown_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                preview_length = 500\n",
    "                print(f\"\\nğŸ“– Preview (first {preview_length} characters):\")\n",
    "                print(\"=\" * 80)\n",
    "                print(content[:preview_length])\n",
    "                print(\"...\")\n",
    "                print(\"=\" * 80)\n",
    "                print(f\"\\nğŸ“Š Total characters: {len(content):,}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Markdown file not found after conversion\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during conversion: {e}\")\n",
    "        print(\"Tip: Make sure Ollama is running and the PDF is accessible\")\n",
    "else:\n",
    "    print(\"âš ï¸ No PDF available for conversion\")\n",
    "    print(\"Skipping this step...\")\n",
    "    markdown_path = None"
   ],
   "id": "pdf_conversion_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Converting PDF: Wegner et al. - 2023 - Complexity measures for EEG microstate sequences - concepts and algorithms.pdf\n",
      "ğŸ“‚ Output folder: example/markdown-library\n",
      "\n",
      "â³ This may take a few minutes depending on PDF size...\n",
      "\n",
      "File is already processed\n",
      "\n",
      "âœ… Conversion successful!\n",
      "ğŸ“ Markdown file: example/markdown-library/Wegner et al. - 2023 - Complexity measures for EEG microstate sequences - concepts and algorithms/Wegner et al. - 2023 - Complexity measures for EEG microstate sequences - concepts and algorithms.md\n",
      "\n",
      "ğŸ“– Preview (first 500 characters):\n",
      "================================================================================\n",
      "![](_page_0_Picture_0.jpeg)\n",
      "\n",
      "# Complexity measures for EEG microstate sequences - concepts and algorithms\n",
      "\n",
      "Frederic von Wegner ( [f.vonwegner@unsw.edu.au \\)](mailto:f.vonwegner@unsw.edu.au) UNSW Sydney Milena Wiemers Klinikum LÃ¼neburg Gesine Hermann Kiel University Inken TÃ¶dt Kiel University Enzo Tagliazucchi University of Buenos Aires Helmut Laufs Kiel University\n",
      "\n",
      "Research Article\n",
      "\n",
      "Keywords:\n",
      "\n",
      "Posted Date: May 10th, 2023\n",
      "\n",
      "DOI: <https://doi.org/10.21203/rs.3.rs-2878411/v1>\n",
      "\n",
      "License: This work is \n",
      "...\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Total characters: 96,317\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 4ï¸âƒ£ Markdown Chunking\n",
    "\n",
    "Large documents need to be split into smaller chunks for effective embedding and retrieval."
   ],
   "id": "chunking_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T14:40:53.088977Z",
     "start_time": "2025-10-22T14:40:49.330168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.Tools.TextSplitter import MarkdownChunker\n",
    "\n",
    "# Use the markdown file we just created (or provide a path to an existing one)\n",
    "if 'markdown_path' in locals() and markdown_path and markdown_path.exists():\n",
    "    print(f\"âœ‚ï¸ Chunking markdown file: {markdown_path.name}\")\n",
    "\n",
    "    # Initialize chunker\n",
    "    chunker = MarkdownChunker(\n",
    "        md_path=str(markdown_path),\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    \n",
    "    # Perform chunking\n",
    "    chunks = chunker.chunk(method='markdown+semantic')\n",
    "\n",
    "    # Add zotero metadata to each chunk\n",
    "    chunks = chunker.add_additional_metadata(metadata=zotero_metadata, splits=chunks)\n",
    "    \n",
    "    print(f\"âœ… Created {len(chunks)} chunks\\n\")\n",
    "    \n",
    "    # Show statistics\n",
    "    chunk_lengths = [c.metadata['length'] for c in chunks]\n",
    "    print(\"ğŸ“Š Chunk Statistics:\")\n",
    "    print(f\"   Min length: {min(chunk_lengths)} chars\")\n",
    "    print(f\"   Max length: {max(chunk_lengths)} chars\")\n",
    "    print(f\"   Avg length: {sum(chunk_lengths) / len(chunk_lengths):.0f} chars\")\n",
    "    \n",
    "    # Display first chunk as example\n",
    "    print(\"\\nğŸ“„ Example Chunk:\")\n",
    "    print(\"=\" * 80)\n",
    "    example_chunk = chunks[0]\n",
    "    print(f\"ID: {example_chunk.metadata['split_id']}\")\n",
    "    print(f\"\\nMetadata:\")\n",
    "    for key, value in example_chunk.metadata.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    print(f\"\\nContent:\\n{example_chunk.page_content}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ No markdown file available for chunking\")\n",
    "    print(\"Skipping this step...\")\n",
    "    chunks = None"
   ],
   "id": "chunking_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸ Chunking markdown file: Wegner et al. - 2023 - Complexity measures for EEG microstate sequences - concepts and algorithms.md\n",
      "âœ… Created 230 chunks\n",
      "\n",
      "ğŸ“Š Chunk Statistics:\n",
      "   Min length: 1 chars\n",
      "   Max length: 5562 chars\n",
      "   Avg length: 409 chars\n",
      "\n",
      "ğŸ“„ Example Chunk:\n",
      "================================================================================\n",
      "ID: 0\n",
      "\n",
      "Metadata:\n",
      "   table: False\n",
      "   split_id: 0\n",
      "   length: 27\n",
      "   title: Complexity measures for EEG microstate sequences - concepts and algorithms\n",
      "   authors: Wegner, Frederic von; Wiemers, Milena; Hermann, Gesine; TÃ¶dt, Inken; Tagliazucchi, Enzo; Laufs, Helmut\n",
      "   year: 2023\n",
      "   doi: 10.21203/rs.3.rs-2878411/v1\n",
      "   url: https://www.researchsquare.com/article/rs-2878411/v1\n",
      "   citation_key: wegnerComplexityMeasuresEEG2023\n",
      "   collections: PCI-From-Resting-State-Reconstruction; scico-test\n",
      "   content_type: pdf-zotero\n",
      "\n",
      "Content:\n",
      "![](_page_0_Picture_0.jpeg)\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 5ï¸âƒ£ Vector Storage with ChromaDB\n",
    "\n",
    "Now we'll create embeddings and store them in a vector database for semantic search."
   ],
   "id": "vector_storage_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T14:40:53.484742Z",
     "start_time": "2025-10-22T14:40:53.140942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.Tools.VectorStorage import ChromaStorage\n",
    "\n",
    "# Initialize ChromaDB storage\n",
    "index_path = os.getenv('INDEX_PATH')\n",
    "collection_name = os.getenv('COLLECTION_NAME')\n",
    "\n",
    "print(f\"ğŸ—„ï¸ Initializing vector storage...\")\n",
    "print(f\"ğŸ“‚ Index path: {index_path}\")\n",
    "print(f\"ğŸ“š Collection: {collection_name}\\n\")\n",
    "\n",
    "storage = ChromaStorage(index_path=index_path, collection_name=collection_name)\n",
    "\n",
    "print(f\"âœ… ChromaDB initialized!\")\n",
    "print(f\"ğŸ“Š Current collection size: {storage.collection.count()} documents\")"
   ],
   "id": "vector_init_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—„ï¸ Initializing vector storage...\n",
      "ğŸ“‚ Index path: example/zotero-vector-storage.db\n",
      "ğŸ“š Collection: scico-test\n",
      "\n",
      "âœ… ChromaDB initialized!\n",
      "ğŸ“Š Current collection size: 0 documents\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ğŸ“¥ Add Chunks to Vector Database"
   ],
   "id": "add_chunks_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T14:40:54.557994Z",
     "start_time": "2025-10-22T14:40:53.495280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add chunks to the vector database (if we have them)\n",
    "if 'chunks' in locals() and chunks:\n",
    "    print(f\"ğŸ“¤ Adding {len(chunks)} chunks to vector database...\")\n",
    "    print(\"â³ Creating embeddings (this may take a moment)...\\n\")\n",
    "    \n",
    "    try:\n",
    "        storage.add_documents(chunks)\n",
    "        print(f\"âœ… Successfully added chunks!\")\n",
    "        print(f\"ğŸ“Š Collection now contains: {storage.collection.count()} documents\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error adding chunks: {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No chunks available to add\")\n",
    "    print(\"You can still query existing documents if the database is not empty\")"
   ],
   "id": "add_chunks_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¤ Adding 230 chunks to vector database...\n",
      "â³ Creating embeddings (this may take a moment)...\n",
      "\n",
      "âœ… Successfully added chunks!\n",
      "ğŸ“Š Collection now contains: 230 documents\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 6ï¸âƒ£ Semantic Search & Retrieval\n",
    "\n",
    "Now comes the magic! Let's query our knowledge base."
   ],
   "id": "retrieval_section"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ğŸ” Simple Query Example"
   ],
   "id": "simple_query_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T14:40:54.638049Z",
     "start_time": "2025-10-22T14:40:54.608525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a query\n",
    "query = \"What is Lemepl Ziv complexity?\"\n",
    "n_results = 5\n",
    "\n",
    "print(f\"ğŸ” Query: '{query}'\")\n",
    "print(f\"ğŸ“Š Retrieving top {n_results} results...\\n\")\n",
    "\n",
    "try:\n",
    "    results = storage.query(query_texts=[query], n_results=n_results)\n",
    "    \n",
    "    if results['documents'] and results['documents'][0]:\n",
    "        print(f\"âœ… Found {len(results['documents'][0])} relevant chunks\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, (doc, metadata, distance) in enumerate(zip(\n",
    "            results['documents'][0],\n",
    "            results['metadatas'][0],\n",
    "            results['distances'][0]\n",
    "        ), 1):\n",
    "            # Calculate similarity score (inverse of distance)\n",
    "            similarity = 1 / (1 + distance)\n",
    "            \n",
    "            print(f\"\\nğŸ“„ Result {i}\")\n",
    "            print(f\"   Similarity: {similarity:.3f} (distance: {distance:.3f})\")\n",
    "            print(f\"   Source: {metadata.get('citation_key', 'Unknown')}\")\n",
    "            print(f\"   Section: {metadata.get('level1', 'N/A')}\")\n",
    "            if metadata.get('level2'):\n",
    "                print(f\"   Subsection: {metadata.get('level2')}\")\n",
    "            print(f\"\\n   Content:\\n   {doc}\")\n",
    "            print(\"\\n\" + \"-\" * 80)\n",
    "    else:\n",
    "        print(\"âš ï¸ No results found. The database might be empty.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during query: {e}\")"
   ],
   "id": "simple_query_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Query: 'What is Lemepl Ziv complexity?'\n",
      "ğŸ“Š Retrieving top 5 results...\n",
      "\n",
      "âœ… Found 5 relevant chunks\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ Result 1\n",
      "   Similarity: 0.640 (distance: 0.563)\n",
      "   Source: wegnerComplexityMeasuresEEG2023\n",
      "   Section: $\\begin{array}{c} 362\\\\ 363 \\end{array} \\quad \\text{Lempel-Ziv complexity (LZC)} \\end{array}$\n",
      "\n",
      "   Content:\n",
      "   Fig. 4 Potts model Lempel-Ziv complexity (LZC) for Q = 4 (A) and Q = 5 (B). The same shape is observed for both models.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“„ Result 2\n",
      "   Similarity: 0.624 (distance: 0.602)\n",
      "   Source: wegnerComplexityMeasuresEEG2023\n",
      "   Section: Complexity measures for EEG microstate sequences - concepts and algorithms\n",
      "\n",
      "   Content:\n",
      "   ., is reproduced by a very short instruction such as 'print A, n times'. As will be explained further below, there are more practical approaches to measure Kolmogorov complexity than trying to find the actual program, namely entropy rate and Lempel-Ziv complexity.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“„ Result 3\n",
      "   Similarity: 0.624 (distance: 0.603)\n",
      "   Source: wegnerComplexityMeasuresEEG2023\n",
      "   Section: Complexity measures for EEG microstate sequences - concepts and algorithms\n",
      "\n",
      "   Content:\n",
      "   087 088 089 090 091 092 One concept is known as algorithmic or Kolmogorov complexity [\\(Alekseev and](#page-23-3) [Yakobson,](#page-23-3) [1981\\)](#page-23-3) and initially defined complexity as the length of the shortest program that is able to reproduce the input data. This measure increases monotonically with the amount of 'randomness' in the data.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“„ Result 4\n",
      "   Similarity: 0.616 (distance: 0.624)\n",
      "   Source: wegnerComplexityMeasuresEEG2023\n",
      "   Section: 916 Microstate sequence complexity in wake and sleep\n",
      "\n",
      "   Content:\n",
      "   <span id=\"page-22-0\"></span>1009 1010 1011 1012 AbÂ´asolo D, da Silva R, Simons S, et al (2014) Lempel-Ziv complexity analysis of local field potentials in different vigilance states with different coarse-graining techniques. In: Romero R (ed) IFMBE Proceedings, XIII Mediterranean Conference on Medical\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“„ Result 5\n",
      "   Similarity: 0.615 (distance: 0.626)\n",
      "   Source: wegnerComplexityMeasuresEEG2023\n",
      "   Section: Complexity measures for EEG microstate sequences - concepts and algorithms\n",
      "\n",
      "   Content:\n",
      "   127 128 129 130 131 132 133 134 135 136 The two complexity concepts discussed can be quantified by a range of metrics that require some disambiguation of the historical terminology. Kolmogorov complexity can be estimated as the randomness of a signal after all correlations have been taken into account (irreducible randomness [\\(Prokopenko et al,](#page-26-2) [2008\\)](#page-26-2)). This is captured by the entropy rate, which is derived from joint entropy (or block entropy) estimates of signal subsequences of different lengths, or via Lempel-Ziv complexity which measures this type of complexity by compressing the signal on the basis of repeated patterns [\\(Lempel and Ziv,](#page-25-3) [1976\\)](#page-25-3).\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ğŸ¯ Interactive Query Tool"
   ],
   "id": "interactive_query_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T14:40:54.672076Z",
     "start_time": "2025-10-22T14:40:54.666336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def search_knowledge_base(query: str, n_results: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Interactive search function with formatted output.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"ğŸ” SEARCH QUERY: {query}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    try:\n",
    "        results = storage.query(query_texts=[query], n_results=n_results)\n",
    "        \n",
    "        if not results['documents'] or not results['documents'][0]:\n",
    "            print(\"âŒ No results found.\")\n",
    "            return\n",
    "        \n",
    "        for i, (doc, metadata, distance) in enumerate(zip(\n",
    "            results['documents'][0],\n",
    "            results['metadatas'][0],\n",
    "            results['distances'][0]\n",
    "        ), 1):\n",
    "            similarity_score = 1 / (1 + distance)\n",
    "            \n",
    "            # Create a visual similarity bar\n",
    "            bar_length = int(similarity_score * 20)\n",
    "            bar = \"â–ˆ\" * bar_length + \"â–‘\" * (20 - bar_length)\n",
    "            \n",
    "            print(f\"\\n{'â–¼' * 40}\")\n",
    "            print(f\"RESULT #{i}\")\n",
    "            print(f\"Relevance: {bar} {similarity_score*100:.1f}%\")\n",
    "            print(f\"\\nğŸ“ Source: {metadata.get('title', 'Unknown')}\")\n",
    "            print(f\"      Key: {metadata.get('citation_key', 'Unknown')}\")\n",
    "            print(f\"ğŸ“– Section: {metadata.get('level1', 'N/A')}\")\n",
    "            if metadata.get('level2'):\n",
    "                print(f\"ğŸ“‘ Subsection: {metadata.get('level2')}\")\n",
    "            \n",
    "            print(f\"\\nğŸ’¡ Content:\")\n",
    "            print(f\"{'â”€' * 80}\")\n",
    "            # Highlight query terms (simple version)\n",
    "            print(doc)\n",
    "            print(f\"{'â”€' * 80}\")\n",
    "        \n",
    "        print(f\"\\n{'=' * 80}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "\n",
    "# Example queries to try\n",
    "example_queries = [\n",
    "    \"What is criticality?\",\n",
    "    \"How is consciousness measured during anesthesia?\",\n",
    "    \"What are the main findings of the study?\",\n",
    "    \"What methods were used in the research?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ Example queries you can try:\")\n",
    "for i, q in enumerate(example_queries, 1):\n",
    "    print(f\"   {i}. {q}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Try running: search_knowledge_base('your question here')\")"
   ],
   "id": "interactive_tool_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Example queries you can try:\n",
      "   1. What is criticality?\n",
      "   2. How is consciousness measured during anesthesia?\n",
      "   3. What are the main findings of the study?\n",
      "   4. What methods were used in the research?\n",
      "\n",
      "ğŸ’¡ Try running: search_knowledge_base('your question here')\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T14:40:54.749594Z",
     "start_time": "2025-10-22T14:40:54.724626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Try your first search!\n",
    "search_knowledge_base(\"What is criticality?\", n_results=3)"
   ],
   "id": "first_search_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ” SEARCH QUERY: What is criticality?\n",
      "================================================================================\n",
      "\n",
      "\n",
      "â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼\n",
      "RESULT #1\n",
      "Relevance: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 56.7%\n",
      "\n",
      "ğŸ“ Source: Complexity measures for EEG microstate sequences - concepts and algorithms\n",
      "      Key: wegnerComplexityMeasuresEEG2023\n",
      "ğŸ“– Section: $\\begin{array}{c} 362\\\\ 363 \\end{array} \\quad \\text{Lempel-Ziv complexity (LZC)} \\end{array}$\n",
      "\n",
      "ğŸ’¡ Content:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "437 438 439 440 441 Excess entropy peaks at the critical point and decays to lower, but non-zero values, away from the critical temperature (T < T<sup>c</sup> and T > Tc). Although asymmetric around the critical temperature, the shape of the excess entropy curve reflects the concept of statistical complexity whereas entropy rate reflects the Kolmogorov complexity concept.\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼\n",
      "RESULT #2\n",
      "Relevance: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 56.6%\n",
      "\n",
      "ğŸ“ Source: Complexity measures for EEG microstate sequences - concepts and algorithms\n",
      "      Key: wegnerComplexityMeasuresEEG2023\n",
      "ğŸ“– Section: Complexity measures for EEG microstate sequences - concepts and algorithms\n",
      "\n",
      "ğŸ’¡ Content:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "**Right**: Above the critical temperature  $(3.0 \\times T_c)$  spatial features 160are apparently random (low order) but a simple independence assumption provides a good statistical 161model for the system, a situation characterized by large Kolmogorov complexity (randomness) and 162low statistical complexity.\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼\n",
      "RESULT #3\n",
      "Relevance: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 55.0%\n",
      "\n",
      "ğŸ“ Source: Complexity measures for EEG microstate sequences - concepts and algorithms\n",
      "      Key: wegnerComplexityMeasuresEEG2023\n",
      "ğŸ“– Section: $\\begin{array}{c} 362\\\\ 363 \\end{array} \\quad \\text{Lempel-Ziv complexity (LZC)} \\end{array}$\n",
      "\n",
      "ğŸ’¡ Content:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "429 430 431 432 433 434 435 436 With increasing temperature the entropy rate rises sigmoidally and the steepest slope occurs at the critical point. At low temperatures (T /T<sup>c</sup> = 0.2), the time courses at most lattice sites are constant, or have very few state changes.\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 7ï¸âƒ£ Using MainProcessor (All-in-One)\n",
    "\n",
    "The `MainProcessor` class provides a convenient wrapper around all components."
   ],
   "id": "main_processor_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T14:40:54.843810Z",
     "start_time": "2025-10-22T14:40:54.779510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from legacy.MainProcessor import MainProcessor\n",
    "\n",
    "# Initialize the main processor\n",
    "processor = MainProcessor(collection_name=os.getenv('COLLECTION_NAME'))\n",
    "\n",
    "print(\"ğŸ¯ MainProcessor initialized!\\n\")\n",
    "print(\"ğŸ“‹ Configuration:\")\n",
    "print(f\"   ğŸ“š Collection: {os.getenv('COLLECTION_NAME')}\")\n",
    "print(f\"   ğŸ“‚ Zotero Library: {processor.zotero_library_path}\")\n",
    "print(f\"   ğŸ“ Markdown Folder: {processor.markdown_folder_path}\")\n",
    "print(f\"   ğŸ’¾ Vector Index: {processor.index_path}\")\n",
    "print(f\"\\n   ğŸ“Š Collection size: {processor.storage.collection.count()} documents\")"
   ],
   "id": "main_processor_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ MainProcessor initialized!\n",
      "\n",
      "ğŸ“‹ Configuration:\n",
      "   ğŸ“š Collection: scico-test\n",
      "   ğŸ“‚ Zotero Library: /home/soenke/Zotero\n",
      "   ğŸ“ Markdown Folder: example/markdown-library\n",
      "   ğŸ’¾ Vector Index: example/zotero-vector-storage.db\n",
      "\n",
      "   ğŸ“Š Collection size: 230 documents\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ğŸ”„ Query Using MainProcessor"
   ],
   "id": "processor_query_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T14:40:54.884979Z",
     "start_time": "2025-10-22T14:40:54.860445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use the processor to query\n",
    "query = \"What are the key findings?\"\n",
    "results = processor.query_vector_storage([query], n_results=3)\n",
    "\n",
    "print(f\"ğŸ” Query: '{query}'\\n\")\n",
    "print(f\"âœ… Retrieved {len(results['documents'][0])} results\\n\")\n",
    "\n",
    "for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):\n",
    "    print(f\"Result {i}: {doc[:150]}...\")\n",
    "    print(f\"Source: {meta.get('filename')}\\n\")"
   ],
   "id": "processor_query_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Query: 'What are the key findings?'\n",
      "\n",
      "âœ… Retrieved 3 results\n",
      "\n",
      "Result 1: The main results of this report can be summarized as follows:...\n",
      "Source: None\n",
      "\n",
      "Result 2: Keywords:...\n",
      "Source: None\n",
      "\n",
      "Result 3: EEG microstate research (Van de Ville et al, 2010; Jia et al, 2021), and fMRI studies(Bullmore et al, 2009; Tagliazucchi et al, 2013)....\n",
      "Source: None\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# ğŸ“ Summary & Next Steps\n",
    "\n",
    "## What We've Learned\n",
    "\n",
    "âœ… **Configuration**: Set up environment variables and verified Ollama  \n",
    "âœ… **Zotero Integration**: Connected to your library and retrieved metadata  \n",
    "âœ… **PDF Processing**: Converted PDFs to structured markdown  \n",
    "âœ… **Chunking**: Split documents into semantic pieces  \n",
    "âœ… **Vector Storage**: Created embeddings with ChromaDB  \n",
    "âœ… **Retrieval**: Performed semantic search on your knowledge base  \n",
    "\n",
    "## ğŸš€ Next Steps\n",
    "\n",
    "1. **Process More Documents**: Run the pipeline on your entire collection\n",
    "2. **Fine-tune Chunking**: Adjust `chunk_size` and `overlap` for better results\n",
    "3. **Build a RAG App**: Add LLM-powered answer generation\n",
    "4. **Create a Web Interface**: Use Streamlit or Gradio for a user-friendly UI\n",
    "5. **Add Query Optimization**: Implement the `RAGQuestionOptimizer` module\n",
    "\n",
    "## ğŸ“š Helpful Functions\n",
    "\n",
    "```python\n",
    "# Search your knowledge base\n",
    "search_knowledge_base(\"your question\", n_results=5)\n",
    "\n",
    "# Get metadata for any PDF\n",
    "retriever.get_metadata_for_pdf(Path(\"path/to/file.pdf\"))\n",
    "\n",
    "# List all PDFs in a collection\n",
    "retriever.get_pdfs_in_collection(\"Collection Name\")\n",
    "\n",
    "# Query using the processor\n",
    "processor.query_vector_storage([\"query\"], n_results=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¤ Contributing\n",
    "\n",
    "This is an evolving project! Future enhancements include:\n",
    "- Query optimization with LLMs\n",
    "- Answer generation with citations\n",
    "- Multi-document synthesis\n",
    "- Advanced RAG techniques\n",
    "\n",
    "Happy researching! ğŸ”¬ğŸ“š"
   ],
   "id": "summary_section"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# ğŸ§ª Experimental: Batch Processing\n",
    "\n",
    "Process multiple PDFs from your collection in one go."
   ],
   "id": "batch_processing_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T14:40:54.920709Z",
     "start_time": "2025-10-22T14:40:54.914985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def batch_process_collection(max_pdfs: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Process multiple PDFs from the collection.\n",
    "    WARNING: This can take a long time!\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”„ Starting batch processing (max {max_pdfs} PDFs)...\\n\")\n",
    "    \n",
    "    pdfs = retriever.get_pdfs_in_collection(os.getenv('COLLECTION_NAME'))\n",
    "    pdfs_to_process = [p for p in pdfs if p['pdf_path']][:max_pdfs]\n",
    "    \n",
    "    total = len(pdfs_to_process)\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for i, pdf in enumerate(pdfs_to_process, 1):\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"Processing {i}/{total}: {pdf['pdf_name']}\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        \n",
    "        try:\n",
    "            # Convert to markdown\n",
    "            print(\"ğŸ“„ Converting to markdown...\")\n",
    "            convert_pdf_to_markdown(\n",
    "                pdf_path=pdf['pdf_path'],\n",
    "                output_path=os.getenv('MARKDOWN_FOLDER_PATH')\n",
    "            )\n",
    "            \n",
    "            # Find markdown file\n",
    "            pdf_stem = Path(pdf['pdf_path']).stem\n",
    "            md_path = Path(os.getenv('MARKDOWN_FOLDER_PATH')) / pdf_stem / f\"{pdf_stem}.md\"\n",
    "            \n",
    "            if md_path.exists():\n",
    "                # Chunk\n",
    "                print(\"âœ‚ï¸ Chunking...\")\n",
    "                chunker = MarkdownChunker(md_path=str(md_path), chunk_size=150, chunk_overlap=50)\n",
    "                chunks = chunker.chunk()\n",
    "                \n",
    "                # Add to vector DB\n",
    "                print(f\"ğŸ“¤ Adding {len(chunks)} chunks to vector DB...\")\n",
    "                storage.add_documents(chunks)\n",
    "                \n",
    "                successful += 1\n",
    "                print(f\"âœ… Success!\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ Markdown file not found\")\n",
    "                failed += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {e}\")\n",
    "            failed += 1\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"ğŸ“Š Batch Processing Complete!\")\n",
    "    print(f\"   âœ… Successful: {successful}\")\n",
    "    print(f\"   âŒ Failed: {failed}\")\n",
    "    print(f\"   ğŸ“š Total documents in DB: {storage.collection.count()}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "# Uncomment to run (WARNING: This will take time!)\n",
    "# batch_process_collection(max_pdfs=3)"
   ],
   "id": "batch_processing_cell",
   "outputs": [],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
