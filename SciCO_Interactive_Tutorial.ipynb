{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üî¨ SciCO - The Zotero Library RAG System\n",
    "\n",
    "Welcome to **SciCO** (Scientific Co-worker)! This interactive tutorial will guide you through:\n",
    "\n",
    "1. **Configuration** - Setting up your environment\n",
    "2. **Zotero Integration** - Connecting to your library and retrieving metadata\n",
    "3. **PDF Processing** - Converting PDFs to searchable markdown\n",
    "4. **Text Chunking** - Breaking documents into semantic pieces\n",
    "5. **Vector Storage** - Creating embeddings for semantic search\n",
    "6. **Retrieval** - Querying your knowledge base\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** combines:\n",
    "- **Vector databases** for semantic search\n",
    "- **Your documents** as the knowledge source\n",
    "- **LLMs** for intelligent question answering\n",
    "\n",
    "This allows you to ask questions about your scientific papers and get answers grounded in your actual sources!"
   ],
   "id": "f316f524255b5f4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## üìã Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "\n",
    "‚úÖ **Ollama** installed and running ([https://ollama.ai](https://ollama.ai))  \n",
    "‚úÖ **Zotero** with a populated library  \n",
    "‚úÖ A `.env` file with required variables (see below)  \n",
    "‚úÖ Python packages installed (`pip install -r requirements.txt`)\n"
   ],
   "id": "prerequisite_section"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 1Ô∏è‚É£ Configuration\n",
    "\n",
    "The project requires a `.env` file in your project root with these variables:"
   ],
   "id": "1c4ed18c0ba44d58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üìù Required Environment Variables"
   ],
   "id": "b7a41fcea696661e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T12:32:44.083343Z",
     "start_time": "2025-10-22T12:32:44.080963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example .env file structure:\n",
    "# Copy this to your .env file and fill in the paths\n",
    "\n",
    "example_env = \"\"\"\n",
    "# Name of the collection in your Zotero library\n",
    "COLLECTION_NAME='Your Collection Name'\n",
    "\n",
    "# Path where markdown files will be saved\n",
    "MARKDOWN_FOLDER_PATH='/path/to/markdown/output'\n",
    "\n",
    "# Path to your Zotero data folder (contains zotero.sqlite)\n",
    "ZOTERO_LIBRARY_PATH='/path/to/Zotero/data'\n",
    "\n",
    "# Path to the ChromaDB index file (should end in .db)\n",
    "INDEX_PATH='/path/to/index/chroma.db'\n",
    "\n",
    "# (Optional) For testing\n",
    "TEST_PDF_PATH='/path/to/test/paper.pdf'\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìÑ Example .env configuration:\")\n",
    "print(example_env)"
   ],
   "id": "e05060f57367368e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Example .env configuration:\n",
      "\n",
      "# Name of the collection in your Zotero library\n",
      "COLLECTION_NAME='Your Collection Name'\n",
      "\n",
      "# Path where markdown files will be saved\n",
      "MARKDOWN_FOLDER_PATH='/path/to/markdown/output'\n",
      "\n",
      "# Path to your Zotero data folder (contains zotero.sqlite)\n",
      "ZOTERO_LIBRARY_PATH='/path/to/Zotero/data'\n",
      "\n",
      "# Path to the ChromaDB index file (should end in .db)\n",
      "INDEX_PATH='/path/to/index/chroma.db'\n",
      "\n",
      "# (Optional) For testing\n",
      "TEST_PDF_PATH='/path/to/test/paper.pdf'\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üîß Import Dependencies and Setup"
   ],
   "id": "import_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T12:32:44.177140Z",
     "start_time": "2025-10-22T12:32:44.127872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import json\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project source to path\n",
    "project_src = Path.cwd()\n",
    "if str(project_src) not in sys.path:\n",
    "    sys.path.insert(0, str(project_src))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Dependencies imported successfully!\")"
   ],
   "id": "bb457cc55e456102",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dependencies imported successfully!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üè• Health Check: Verify Ollama is Running"
   ],
   "id": "health_check_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T12:32:44.193813Z",
     "start_time": "2025-10-22T12:32:44.184123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ensure_ollama_running(host: str = \"127.0.0.1\", port: int = 11434, timeout: float = 2.0) -> dict:\n",
    "    \"\"\"Check if Ollama is running and return status info.\"\"\"\n",
    "    base_url = f\"http://{host}:{port}\"\n",
    "    try:\n",
    "        resp = requests.get(f\"{base_url}/api/version\", timeout=timeout)\n",
    "        if resp.status_code == 200:\n",
    "            version_info = resp.json()\n",
    "            return {\n",
    "                'status': 'running',\n",
    "                'url': base_url,\n",
    "                'version': version_info.get('version', 'unknown')\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'message': f\"Ollama responded with status {resp.status_code}\"\n",
    "            }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\n",
    "            'status': 'not_running',\n",
    "            'message': f\"Cannot reach Ollama at {base_url}\",\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "# Run health check\n",
    "ollama_status = ensure_ollama_running()\n",
    "\n",
    "if ollama_status['status'] == 'running':\n",
    "    display(HTML(\n",
    "        f\"<div style='padding:10px; background-color:#1a4d2e; border-left:4px solid #28a745; border-radius:4px; color:#ffffff;'>\"\n",
    "        f\"<strong>‚úÖ Ollama is running!</strong><br>\"\n",
    "        f\"üîó URL: {ollama_status['url']}<br>\"\n",
    "        f\"üì¶ Version: {ollama_status['version']}\"\n",
    "        f\"</div>\"))\n",
    "else:\n",
    "    display(HTML(\n",
    "        f\"<div style='padding:10px; background-color:#5c1a1a; border-left:4px solid #dc3545; border-radius:4px; color:#ffffff;'>\"\n",
    "        f\"<strong>‚ùå Ollama is NOT running!</strong><br>\"\n",
    "        f\"‚ö†Ô∏è {ollama_status.get('message', 'Unknown error')}<br>\"\n",
    "        f\"<em>Please start Ollama before continuing.</em>\"\n",
    "        f\"</div>\"))\n",
    "    raise RuntimeError(\"Ollama must be running to use this notebook.\")\n"
   ],
   "id": "1b5da5ba20bbbb07",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div style='padding:10px; background-color:#1a4d2e; border-left:4px solid #28a745; border-radius:4px; color:#ffffff;'><strong>‚úÖ Ollama is running!</strong><br>üîó URL: http://127.0.0.1:11434<br>üì¶ Version: 0.12.5</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üìä Verify Configuration"
   ],
   "id": "verify_config_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T12:32:44.239366Z",
     "start_time": "2025-10-22T12:32:44.236349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if all required environment variables are set\n",
    "required_vars = ['COLLECTION_NAME', 'MARKDOWN_FOLDER_PATH', 'ZOTERO_LIBRARY_PATH', 'INDEX_PATH']\n",
    "config_status = {}\n",
    "\n",
    "print(\"üîç Configuration Status:\\n\")\n",
    "for var in required_vars:\n",
    "    value = os.getenv(var)\n",
    "    config_status[var] = value\n",
    "    status = \"‚úÖ\" if value else \"‚ùå\"\n",
    "    print(f\"{status} {var}: {value if value else 'NOT SET'}\")\n",
    "\n",
    "all_set = all(config_status.values())\n",
    "if all_set:\n",
    "    print(\"\\n‚úÖ All required variables are configured!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some variables are missing. Please update your .env file.\")"
   ],
   "id": "verify_config_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Configuration Status:\n",
      "\n",
      "‚úÖ COLLECTION_NAME: scico-test\n",
      "‚úÖ MARKDOWN_FOLDER_PATH: explore_langchain\n",
      "‚úÖ ZOTERO_LIBRARY_PATH: /home/soenke/Zotero\n",
      "‚úÖ INDEX_PATH: explore_langchain/test.db\n",
      "\n",
      "‚úÖ All required variables are configured!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 2Ô∏è‚É£ Zotero Integration\n",
    "\n",
    "Let's connect to your Zotero database and explore what's inside!"
   ],
   "id": "7cdf4e67339e51a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T12:32:44.547195Z",
     "start_time": "2025-10-22T12:32:44.285182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.ZoteroIntegration import ZoteroMetadataRetriever\n",
    "\n",
    "# Initialize the Zotero connection\n",
    "zotero_path = Path(os.getenv('ZOTERO_LIBRARY_PATH'))\n",
    "retriever = ZoteroMetadataRetriever(zotero_path)\n",
    "\n",
    "print(\"üîå Connecting to Zotero database...\")\n",
    "retriever.initialize()\n",
    "print(\"‚úÖ Connected successfully!\")\n",
    "print(f\"üìÇ Database path: {retriever.config.sqlite_path}\")"
   ],
   "id": "zotero_connect_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Connecting to Zotero database...\n",
      "‚úÖ Connected successfully!\n",
      "üìÇ Database path: /home/soenke/Zotero/zotero.sqlite\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üìö Explore Collections and PDFs"
   ],
   "id": "explore_collections_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T12:32:44.653505Z",
     "start_time": "2025-10-22T12:32:44.553749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get PDFs from the configured collection\n",
    "collection_name = os.getenv('COLLECTION_NAME')\n",
    "print(f\"üìñ Retrieving PDFs from collection: '{collection_name}'\\n\")\n",
    "\n",
    "pdfs = retriever.get_pdfs_in_collection(collection_name)\n",
    "\n",
    "if pdfs:\n",
    "    print(f\"‚úÖ Found {len(pdfs)} PDF(s) in this collection\\n\")\n",
    "    print(\"üìÑ First 3 PDFs:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, pdf in enumerate(pdfs[:3], 1):\n",
    "        print(f\"\\n{i}. {pdf['pdf_name']}\")\n",
    "        print(f\"   Citation Key: {pdf['citationkey'] or 'None'}\")\n",
    "        print(f\"   Item ID: {pdf['itemID']}\")\n",
    "        print(f\"   Path: {pdf['pdf_path'] or 'Not found in storage'}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No PDFs found in collection '{collection_name}'\")\n",
    "    print(\"Tip: Make sure your collection name matches exactly (case-sensitive)\")"
   ],
   "id": "get_pdfs_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Retrieving PDFs from collection: 'scico-test'\n",
      "\n",
      "‚úÖ Found 1 PDF(s) in this collection\n",
      "\n",
      "üìÑ First 3 PDFs:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Wegner et al. - 2023 - Complexity measures for EEG microstate sequences - concepts and algorithms.pdf\n",
      "   Citation Key: wegnerComplexityMeasuresEEG2023\n",
      "   Item ID: 289\n",
      "   Path: /home/soenke/Zotero/storage/9N8E7TQU/Wegner et al. - 2023 - Complexity measures for EEG microstate sequences - concepts and algorithms.pdf\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üîç Deep Dive: Get Full Metadata for a PDF"
   ],
   "id": "metadata_deep_dive_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T12:32:44.688769Z",
     "start_time": "2025-10-22T12:32:44.662592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's examine the full metadata for the first PDF (if available)\n",
    "if pdfs and pdfs[0]['pdf_path']:\n",
    "    sample_pdf_path = Path(pdfs[0]['pdf_path'])\n",
    "    print(f\"üî¨ Analyzing: {sample_pdf_path.name}\\n\")\n",
    "    \n",
    "    metadata = retriever.get_metadata_for_pdf(sample_pdf_path)\n",
    "    \n",
    "    if metadata:\n",
    "        print(\"üìä Full Metadata:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nüìñ Title: {metadata.get('title', 'N/A')}\")\n",
    "        print(f\"\\n‚úçÔ∏è Authors: {metadata.get('authors', 'N/A')}\")\n",
    "        print(f\"\\nüìÖ Year: {metadata.get('year', 'N/A')}\")\n",
    "        print(f\"\\nüîó DOI: {metadata.get('doi', 'N/A')}\")\n",
    "        print(f\"\\nüåê URL: {metadata.get('url', 'N/A')}\")\n",
    "        print(f\"\\nüì∞ Publication: {metadata.get('publication_title', 'N/A')}\")\n",
    "        \n",
    "        if metadata.get('abstract'):\n",
    "            abstract = metadata['abstract']\n",
    "            print(f\"\\nüìù Abstract: {abstract[:200]}...\" if len(abstract) > 200 else f\"\\nüìù Abstract: {abstract}\")\n",
    "        \n",
    "        if metadata.get('tags'):\n",
    "            print(f\"\\nüè∑Ô∏è Tags: {', '.join(metadata['tags'])}\")\n",
    "        \n",
    "        if metadata.get('collections'):\n",
    "            print(f\"\\nüìÅ Collections:\")\n",
    "            for coll in metadata['collections']:\n",
    "                print(f\"   - {coll['name']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Could not retrieve metadata\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No valid PDF path found for analysis\")\n",
    "    print(\"Tip: Make sure PDFs exist in your Zotero storage folder\")"
   ],
   "id": "metadata_details_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Analyzing: Wegner et al. - 2023 - Complexity measures for EEG microstate sequences - concepts and algorithms.pdf\n",
      "\n",
      "üìä Full Metadata:\n",
      "================================================================================\n",
      "\n",
      "üìñ Title: Complexity measures for EEG microstate sequences - concepts and algorithms\n",
      "\n",
      "‚úçÔ∏è Authors: Wegner, Frederic von; Wiemers, Milena; Hermann, Gesine; T√∂dt, Inken; Tagliazucchi, Enzo; Laufs, Helmut\n",
      "\n",
      "üìÖ Year: 2023\n",
      "\n",
      "üîó DOI: 10.21203/rs.3.rs-2878411/v1\n",
      "\n",
      "üåê URL: https://www.researchsquare.com/article/rs-2878411/v1\n",
      "\n",
      "üì∞ Publication: None\n",
      "\n",
      "üìù Abstract: EEG microstate sequence analysis quantifies properties of ongoing brain electrical activity which is known to exhibit complex dynamics across many time scales. In this report we review recent developm...\n",
      "\n",
      "üìÅ Collections:\n",
      "   - PCI-From-Resting-State-Reconstruction\n",
      "   - scico-test\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 3Ô∏è‚É£ PDF to Markdown Conversion\n",
    "\n",
    "Now let's convert a PDF to structured Markdown using the `marker` library with Ollama."
   ],
   "id": "pdf_conversion_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T12:36:14.478170Z",
     "start_time": "2025-10-22T12:36:14.472541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.PdfToMarkdown import convert_pdf_to_markdown\n",
    "\n",
    "# We'll use the first PDF from our collection (if available)\n",
    "if pdfs and pdfs[0]['pdf_path']:\n",
    "    pdf_path = pdfs[0]['pdf_path']\n",
    "    output_folder = os.getenv('MARKDOWN_FOLDER_PATH')\n",
    "    \n",
    "    print(f\"üìÑ Converting PDF: {Path(pdf_path).name}\")\n",
    "    print(f\"üìÇ Output folder: {output_folder}\")\n",
    "    print(\"\\n‚è≥ This may take a few minutes depending on PDF size...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        # Find the generated markdown file\n",
    "        pdf_name = Path(pdf_path).stem\n",
    "        markdown_path = Path(output_folder) / pdf_name / f\"{pdf_name}.md\"\n",
    "\n",
    "        # Convert PDF to markdown\n",
    "        try:\n",
    "            convert_pdf_to_markdown(pdf_path=pdf_path, output_path=output_folder)\n",
    "        except FileExistsError as e:\n",
    "            print('File is already processed')\n",
    "\n",
    "        if markdown_path.exists():\n",
    "            print(f\"\\n‚úÖ Conversion successful!\")\n",
    "            print(f\"üìù Markdown file: {markdown_path}\")\n",
    "            \n",
    "            # Show a preview\n",
    "            with open(markdown_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                preview_length = 500\n",
    "                print(f\"\\nüìñ Preview (first {preview_length} characters):\")\n",
    "                print(\"=\" * 80)\n",
    "                print(content[:preview_length])\n",
    "                print(\"...\")\n",
    "                print(\"=\" * 80)\n",
    "                print(f\"\\nüìä Total characters: {len(content):,}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Markdown file not found after conversion\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during conversion: {e}\")\n",
    "        print(\"Tip: Make sure Ollama is running and the PDF is accessible\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No PDF available for conversion\")\n",
    "    print(\"Skipping this step...\")\n",
    "    markdown_path = None"
   ],
   "id": "pdf_conversion_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Converting PDF: Wegner et al. - 2023 - Complexity measures for EEG microstate sequences - concepts and algorithms.pdf\n",
      "üìÇ Output folder: explore_langchain\n",
      "\n",
      "‚è≥ This may take a few minutes depending on PDF size...\n",
      "\n",
      "File is already processed\n",
      "\n",
      "‚úÖ Conversion successful!\n",
      "üìù Markdown file: explore_langchain/Wegner et al. - 2023 - Complexity measures for EEG microstate sequences - concepts and algorithms/Wegner et al. - 2023 - Complexity measures for EEG microstate sequences - concepts and algorithms.md\n",
      "\n",
      "üìñ Preview (first 500 characters):\n",
      "================================================================================\n",
      "![](_page_0_Picture_0.jpeg)\n",
      "\n",
      "# Complexity measures for EEG microstate sequences - concepts and algorithms\n",
      "\n",
      "Frederic von Wegner ( [f.vonwegner@unsw.edu.au \\)](mailto:f.vonwegner@unsw.edu.au) UNSW Sydney Milena Wiemers Klinikum L√ºneburg Gesine Hermann Kiel University Inken T√∂dt Kiel University Enzo Tagliazucchi University of Buenos Aires Helmut Laufs Kiel University\n",
      "\n",
      "Research Article\n",
      "\n",
      "Keywords:\n",
      "\n",
      "Posted Date: May 10th, 2023\n",
      "\n",
      "DOI: <https://doi.org/10.21203/rs.3.rs-2878411/v1>\n",
      "\n",
      "License: This work is \n",
      "...\n",
      "================================================================================\n",
      "\n",
      "üìä Total characters: 96,317\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 4Ô∏è‚É£ Markdown Chunking\n",
    "\n",
    "Large documents need to be split into smaller chunks for effective embedding and retrieval."
   ],
   "id": "chunking_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T12:36:28.716616Z",
     "start_time": "2025-10-22T12:36:28.700299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.MarkdownChunker import MarkdownChunker\n",
    "\n",
    "# Use the markdown file we just created (or provide a path to an existing one)\n",
    "if 'markdown_path' in locals() and markdown_path and markdown_path.exists():\n",
    "    print(f\"‚úÇÔ∏è Chunking markdown file: {markdown_path.name}\")\n",
    "\n",
    "    # Initialize chunker\n",
    "    chunker = MarkdownChunker(\n",
    "        md_path=str(markdown_path),\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    \n",
    "    # Perform chunking\n",
    "    chunks = chunker.chunk(method='markdown+recursive')\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(chunks)} chunks\\n\")\n",
    "    \n",
    "    # Show statistics\n",
    "    chunk_lengths = [c.metadata['length'] for c in chunks]\n",
    "    print(\"üìä Chunk Statistics:\")\n",
    "    print(f\"   Min length: {min(chunk_lengths)} chars\")\n",
    "    print(f\"   Max length: {max(chunk_lengths)} chars\")\n",
    "    print(f\"   Avg length: {sum(chunk_lengths) / len(chunk_lengths):.0f} chars\")\n",
    "    \n",
    "    # Display first chunk as example\n",
    "    print(\"\\nüìÑ Example Chunk:\")\n",
    "    print(\"=\" * 80)\n",
    "    example_chunk = chunks[0]\n",
    "    print(f\"ID: {example_chunk.metadata['split_id']}\")\n",
    "    print(f\"\\nMetadata:\")\n",
    "    for key, value in example_chunk.metadata.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    print(f\"\\nContent:\\n{example_chunk.page_content}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No markdown file available for chunking\")\n",
    "    print(\"Skipping this step...\")\n",
    "    chunks = None"
   ],
   "id": "chunking_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Chunking markdown file: Wegner et al. - 2023 - Complexity measures for EEG microstate sequences - concepts and algorithms.md\n",
      "‚úÖ Created 347 chunks\n",
      "\n",
      "üìä Chunk Statistics:\n",
      "   Min length: 1 chars\n",
      "   Max length: 853 chars\n",
      "   Avg length: 275 chars\n",
      "\n",
      "üìÑ Example Chunk:\n",
      "================================================================================\n",
      "ID: 0\n",
      "\n",
      "Metadata:\n",
      "   table: False\n",
      "   split_id: 0\n",
      "   length: 27\n",
      "\n",
      "Content:\n",
      "![](_page_0_Picture_0.jpeg)\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 5Ô∏è‚É£ Vector Storage with ChromaDB\n",
    "\n",
    "Now we'll create embeddings and store them in a vector database for semantic search."
   ],
   "id": "vector_storage_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T12:36:36.738895Z",
     "start_time": "2025-10-22T12:36:36.718719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.VectorStorage import ChromaStorage\n",
    "\n",
    "# Initialize ChromaDB storage\n",
    "index_path = os.getenv('INDEX_PATH')\n",
    "collection_name = os.getenv('COLLECTION_NAME')\n",
    "\n",
    "print(f\"üóÑÔ∏è Initializing vector storage...\")\n",
    "print(f\"üìÇ Index path: {index_path}\")\n",
    "print(f\"üìö Collection: {collection_name}\\n\")\n",
    "\n",
    "storage = ChromaStorage(index_path=index_path, collection_name=collection_name)\n",
    "\n",
    "print(f\"‚úÖ ChromaDB initialized!\")\n",
    "print(f\"üìä Current collection size: {storage.collection.count()} documents\")"
   ],
   "id": "vector_init_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÑÔ∏è Initializing vector storage...\n",
      "üìÇ Index path: explore_langchain/test.db\n",
      "üìö Collection: scico-test\n",
      "\n",
      "‚úÖ ChromaDB initialized!\n",
      "üìä Current collection size: 0 documents\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üì• Add Chunks to Vector Database"
   ],
   "id": "add_chunks_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T12:36:41.584980Z",
     "start_time": "2025-10-22T12:36:40.034921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add chunks to the vector database (if we have them)\n",
    "if 'chunks' in locals() and chunks:\n",
    "    print(f\"üì§ Adding {len(chunks)} chunks to vector database...\")\n",
    "    print(\"‚è≥ Creating embeddings (this may take a moment)...\\n\")\n",
    "    \n",
    "    try:\n",
    "        storage.add_documents(chunks)\n",
    "        print(f\"‚úÖ Successfully added chunks!\")\n",
    "        print(f\"üìä Collection now contains: {storage.collection.count()} documents\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error adding chunks: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No chunks available to add\")\n",
    "    print(\"You can still query existing documents if the database is not empty\")"
   ],
   "id": "add_chunks_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Adding 347 chunks to vector database...\n",
      "‚è≥ Creating embeddings (this may take a moment)...\n",
      "\n",
      "‚úÖ Successfully added chunks!\n",
      "üìä Collection now contains: 347 documents\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 6Ô∏è‚É£ Semantic Search & Retrieval\n",
    "\n",
    "Now comes the magic! Let's query our knowledge base."
   ],
   "id": "retrieval_section"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üîç Simple Query Example"
   ],
   "id": "simple_query_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T12:39:01.565817Z",
     "start_time": "2025-10-22T12:39:01.524220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a query\n",
    "query = \"What is criticality in EEG signals?\"\n",
    "n_results = 3\n",
    "\n",
    "print(f\"üîç Query: '{query}'\")\n",
    "print(f\"üìä Retrieving top {n_results} results...\\n\")\n",
    "\n",
    "try:\n",
    "    results = storage.query(query_texts=[query], n_results=n_results)\n",
    "    \n",
    "    if results['documents'] and results['documents'][0]:\n",
    "        print(f\"‚úÖ Found {len(results['documents'][0])} relevant chunks\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, (doc, metadata, distance) in enumerate(zip(\n",
    "            results['documents'][0],\n",
    "            results['metadatas'][0],\n",
    "            results['distances'][0]\n",
    "        ), 1):\n",
    "            # Calculate similarity score (inverse of distance)\n",
    "            similarity = 1 / (1 + distance)\n",
    "            \n",
    "            print(f\"\\nüìÑ Result {i}\")\n",
    "            print(f\"   Similarity: {similarity:.3f} (distance: {distance:.3f})\")\n",
    "            print(f\"   Source: {metadata.get('citationkey', 'Unknown')}\")\n",
    "            print(f\"   Section: {metadata.get('level1', 'N/A')}\")\n",
    "            if metadata.get('level2'):\n",
    "                print(f\"   Subsection: {metadata.get('level2')}\")\n",
    "            print(f\"\\n   Content:\\n   {doc[:300]}...\" if len(doc) > 300 else f\"\\n   Content:\\n   {doc}\")\n",
    "            print(\"\\n\" + \"-\" * 80)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No results found. The database might be empty.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during query: {e}\")"
   ],
   "id": "simple_query_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: 'What is criticality in EEG signals?'\n",
      "üìä Retrieving top 3 results...\n",
      "\n",
      "‚úÖ Found 3 relevant chunks\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÑ Result 1\n",
      "   Similarity: 0.661 (distance: 0.513)\n",
      "   Source: Unknown\n",
      "   Section: 916 Microstate sequence complexity in wake and sleep\n",
      "\n",
      "   Content:\n",
      "   967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 might explain why electrophysiological and imaging data are unable to give a unique answer to the question how close the brain is to a critical state in a defined condition. Another difference between...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìÑ Result 2\n",
      "   Similarity: 0.659 (distance: 0.518)\n",
      "   Source: Unknown\n",
      "   Section: Complexity measures for EEG microstate sequences - concepts and algorithms\n",
      "\n",
      "   Content:\n",
      "   \n",
      "| Complexity measures for EEG microstate                                                                                                                                                                                                                                                                   ...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìÑ Result 3\n",
      "   Similarity: 0.644 (distance: 0.552)\n",
      "   Source: Unknown\n",
      "   Section: 916 Microstate sequence complexity in wake and sleep\n",
      "\n",
      "   Content:\n",
      "   . In the past we have observed smaller Hurst exponents during sleep in regional BOLD (blood oxygen level dependent) signals from functional MRI data, and interpreted those as a departure from criticality, concluding that wakefulness was the state closest to criticality [\\(Tagliazucchi et al,](#page-...\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üéØ Interactive Query Tool"
   ],
   "id": "interactive_query_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T12:32:54.835507Z",
     "start_time": "2025-10-22T12:32:54.830452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def search_knowledge_base(query: str, n_results: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Interactive search function with formatted output.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"üîç SEARCH QUERY: {query}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    try:\n",
    "        results = storage.query(query_texts=[query], n_results=n_results)\n",
    "        \n",
    "        if not results['documents'] or not results['documents'][0]:\n",
    "            print(\"‚ùå No results found.\")\n",
    "            return\n",
    "        \n",
    "        for i, (doc, metadata, distance) in enumerate(zip(\n",
    "            results['documents'][0],\n",
    "            results['metadatas'][0],\n",
    "            results['distances'][0]\n",
    "        ), 1):\n",
    "            similarity_score = 1 / (1 + distance)\n",
    "            \n",
    "            # Create a visual similarity bar\n",
    "            bar_length = int(similarity_score * 20)\n",
    "            bar = \"‚ñà\" * bar_length + \"‚ñë\" * (20 - bar_length)\n",
    "            \n",
    "            print(f\"\\n{'‚ñº' * 40}\")\n",
    "            print(f\"RESULT #{i}\")\n",
    "            print(f\"Relevance: {bar} {similarity_score*100:.1f}%\")\n",
    "            print(f\"\\nüìÅ Source: {metadata.get('filename', 'Unknown')}\")\n",
    "            print(f\"üìñ Section: {metadata.get('level1', 'N/A')}\")\n",
    "            if metadata.get('level2'):\n",
    "                print(f\"üìë Subsection: {metadata.get('level2')}\")\n",
    "            \n",
    "            print(f\"\\nüí° Content Preview:\")\n",
    "            print(f\"{'‚îÄ' * 80}\")\n",
    "            # Highlight query terms (simple version)\n",
    "            preview = doc[:400] + \"...\" if len(doc) > 400 else doc\n",
    "            print(preview)\n",
    "            print(f\"{'‚îÄ' * 80}\")\n",
    "        \n",
    "        print(f\"\\n{'=' * 80}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Example queries to try\n",
    "example_queries = [\n",
    "    \"What is criticality?\",\n",
    "    \"How is consciousness measured during anesthesia?\",\n",
    "    \"What are the main findings of the study?\",\n",
    "    \"What methods were used in the research?\"\n",
    "]\n",
    "\n",
    "print(\"üìù Example queries you can try:\")\n",
    "for i, q in enumerate(example_queries, 1):\n",
    "    print(f\"   {i}. {q}\")\n",
    "\n",
    "print(\"\\nüí° Try running: search_knowledge_base('your question here')\")"
   ],
   "id": "interactive_tool_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Example queries you can try:\n",
      "   1. What is criticality?\n",
      "   2. How is consciousness measured during anesthesia?\n",
      "   3. What are the main findings of the study?\n",
      "   4. What methods were used in the research?\n",
      "\n",
      "üí° Try running: search_knowledge_base('your question here')\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T12:32:54.915998Z",
     "start_time": "2025-10-22T12:32:54.884126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Try your first search!\n",
    "search_knowledge_base(\"What is criticality?\", n_results=3)"
   ],
   "id": "first_search_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç SEARCH QUERY: What is criticality?\n",
      "================================================================================\n",
      "\n",
      "‚ùå No results found.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 7Ô∏è‚É£ Using MainProcessor (All-in-One)\n",
    "\n",
    "The `MainProcessor` class provides a convenient wrapper around all components."
   ],
   "id": "main_processor_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T12:32:55.005799Z",
     "start_time": "2025-10-22T12:32:54.935235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.MainProcessor import MainProcessor\n",
    "\n",
    "# Initialize the main processor\n",
    "processor = MainProcessor(collection_name=os.getenv('COLLECTION_NAME'))\n",
    "\n",
    "print(\"üéØ MainProcessor initialized!\\n\")\n",
    "print(\"üìã Configuration:\")\n",
    "print(f\"   üìö Collection: {os.getenv('COLLECTION_NAME')}\")\n",
    "print(f\"   üìÇ Zotero Library: {processor.zotero_library_path}\")\n",
    "print(f\"   üìù Markdown Folder: {processor.markdown_folder_path}\")\n",
    "print(f\"   üíæ Vector Index: {processor.index_path}\")\n",
    "print(f\"\\n   üìä Collection size: {processor.storage.collection.count()} documents\")"
   ],
   "id": "main_processor_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ MainProcessor initialized!\n",
      "\n",
      "üìã Configuration:\n",
      "   üìö Collection: scico-test\n",
      "   üìÇ Zotero Library: /home/soenke/Zotero\n",
      "   üìù Markdown Folder: explore_langchain\n",
      "   üíæ Vector Index: explore_langchain/test.db\n",
      "\n",
      "   üìä Collection size: 0 documents\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üîÑ Query Using MainProcessor"
   ],
   "id": "processor_query_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T12:32:55.054758Z",
     "start_time": "2025-10-22T12:32:55.022425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use the processor to query\n",
    "query = \"What are the key findings?\"\n",
    "results = processor.query_vector_storage([query], n_results=3)\n",
    "\n",
    "print(f\"üîç Query: '{query}'\\n\")\n",
    "print(f\"‚úÖ Retrieved {len(results['documents'][0])} results\\n\")\n",
    "\n",
    "for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0]), 1):\n",
    "    print(f\"Result {i}: {doc[:150]}...\")\n",
    "    print(f\"Source: {meta.get('filename')}\\n\")"
   ],
   "id": "processor_query_cell",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: 'What are the key findings?'\n",
      "\n",
      "‚úÖ Retrieved 0 results\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# üéì Summary & Next Steps\n",
    "\n",
    "## What We've Learned\n",
    "\n",
    "‚úÖ **Configuration**: Set up environment variables and verified Ollama  \n",
    "‚úÖ **Zotero Integration**: Connected to your library and retrieved metadata  \n",
    "‚úÖ **PDF Processing**: Converted PDFs to structured markdown  \n",
    "‚úÖ **Chunking**: Split documents into semantic pieces  \n",
    "‚úÖ **Vector Storage**: Created embeddings with ChromaDB  \n",
    "‚úÖ **Retrieval**: Performed semantic search on your knowledge base  \n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "1. **Process More Documents**: Run the pipeline on your entire collection\n",
    "2. **Fine-tune Chunking**: Adjust `chunk_size` and `overlap` for better results\n",
    "3. **Build a RAG App**: Add LLM-powered answer generation\n",
    "4. **Create a Web Interface**: Use Streamlit or Gradio for a user-friendly UI\n",
    "5. **Add Query Optimization**: Implement the `RAGQuestionOptimizer` module\n",
    "\n",
    "## üìö Helpful Functions\n",
    "\n",
    "```python\n",
    "# Search your knowledge base\n",
    "search_knowledge_base(\"your question\", n_results=5)\n",
    "\n",
    "# Get metadata for any PDF\n",
    "retriever.get_metadata_for_pdf(Path(\"path/to/file.pdf\"))\n",
    "\n",
    "# List all PDFs in a collection\n",
    "retriever.get_pdfs_in_collection(\"Collection Name\")\n",
    "\n",
    "# Query using the processor\n",
    "processor.query_vector_storage([\"query\"], n_results=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù Contributing\n",
    "\n",
    "This is an evolving project! Future enhancements include:\n",
    "- Query optimization with LLMs\n",
    "- Answer generation with citations\n",
    "- Multi-document synthesis\n",
    "- Advanced RAG techniques\n",
    "\n",
    "Happy researching! üî¨üìö"
   ],
   "id": "summary_section"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# üß™ Experimental: Batch Processing\n",
    "\n",
    "Process multiple PDFs from your collection in one go."
   ],
   "id": "batch_processing_section"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T12:32:55.079462Z",
     "start_time": "2025-10-22T12:32:55.073160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def batch_process_collection(max_pdfs: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Process multiple PDFs from the collection.\n",
    "    WARNING: This can take a long time!\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Starting batch processing (max {max_pdfs} PDFs)...\\n\")\n",
    "    \n",
    "    pdfs = retriever.get_pdfs_in_collection(os.getenv('COLLECTION_NAME'))\n",
    "    pdfs_to_process = [p for p in pdfs if p['pdf_path']][:max_pdfs]\n",
    "    \n",
    "    total = len(pdfs_to_process)\n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for i, pdf in enumerate(pdfs_to_process, 1):\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"Processing {i}/{total}: {pdf['pdf_name']}\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        \n",
    "        try:\n",
    "            # Convert to markdown\n",
    "            print(\"üìÑ Converting to markdown...\")\n",
    "            convert_pdf_to_markdown(\n",
    "                pdf_path=pdf['pdf_path'],\n",
    "                output_path=os.getenv('MARKDOWN_FOLDER_PATH')\n",
    "            )\n",
    "            \n",
    "            # Find markdown file\n",
    "            pdf_stem = Path(pdf['pdf_path']).stem\n",
    "            md_path = Path(os.getenv('MARKDOWN_FOLDER_PATH')) / pdf_stem / f\"{pdf_stem}.md\"\n",
    "            \n",
    "            if md_path.exists():\n",
    "                # Chunk\n",
    "                print(\"‚úÇÔ∏è Chunking...\")\n",
    "                chunker = MarkdownChunker(md_path=str(md_path), chunk_size=150, chunk_overlap=50)\n",
    "                chunks = chunker.chunk()\n",
    "                \n",
    "                # Add to vector DB\n",
    "                print(f\"üì§ Adding {len(chunks)} chunks to vector DB...\")\n",
    "                storage.add_documents(chunks)\n",
    "                \n",
    "                successful += 1\n",
    "                print(f\"‚úÖ Success!\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Markdown file not found\")\n",
    "                failed += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            failed += 1\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"üìä Batch Processing Complete!\")\n",
    "    print(f\"   ‚úÖ Successful: {successful}\")\n",
    "    print(f\"   ‚ùå Failed: {failed}\")\n",
    "    print(f\"   üìö Total documents in DB: {storage.collection.count()}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "# Uncomment to run (WARNING: This will take time!)\n",
    "# batch_process_collection(max_pdfs=3)"
   ],
   "id": "batch_processing_cell",
   "outputs": [],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
